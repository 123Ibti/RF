import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from skopt import BayesSearchCV
from sklearn.metrics import f1_score
from sklearn.impute import SimpleImputer

# Load CSV file (assuming file is named "data.csv")
data = pd.read_csv("/content/normalized_TD2.csv")

# Use columns '0', '1', '2' as predictors and 'Label' as the target variable.
# Convert predictor columns to 32-bit floats to reduce memory usage.
X = data[['0', '1', '2']].astype(np.float32)
y = data['Label']

# Check class distribution in y
print("Original class distribution:")
print(y.value_counts())

# Identify and remove rare classes (those with fewer than 2 samples)
rare_classes = y.value_counts()[y.value_counts() < 2].index.tolist()
if rare_classes:
    print("Removing rare classes with <2 samples:", rare_classes)
    mask = ~y.isin(rare_classes)
    X = X[mask]
    y = y[mask]
    print("Updated class distribution:")
    print(y.value_counts())

# 80/20 train-test split with stratification
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# Impute missing values in training and test sets using mean strategy
imputer = SimpleImputer(strategy='mean')
X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns).astype(np.float32)
X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns).astype(np.float32)

# Apply SMOTE to balance the training set
smote = SMOTE(random_state=42, k_neighbors=1)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

# Define reduced hyperparameter search space for faster training
param_space = {
    'n_estimators': (50, 150),    # Fewer trees for speed
    'max_depth': (3, 20),         # Limit tree depth to reduce overfitting
    'min_samples_split': (2, 5),  # Fewer options to speed up search
    'min_samples_leaf': (1, 5),   # Reduce leaf size range for simplicity
    'max_features': ['sqrt', 'log2', None]
}

# Initialize the RandomForestClassifier
rf = RandomForestClassifier(random_state=42, n_jobs=-1)

# Setup Bayesian hyperparameter optimization (fewer iterations for speed)
opt = BayesSearchCV(
    estimator=rf,
    search_spaces=param_space,
    scoring='f1_macro',
    cv=3,
    n_iter=15,   # Reduced iterations for faster tuning
    n_jobs=1,
    random_state=42
)

# Fit the model on the balanced training data
opt.fit(X_train_res, y_train_res)

# Evaluate the optimized model on the test set
y_pred = opt.predict(X_test)
test_f1 = f1_score(y_test, y_pred, average='macro')
print("Test f1_macro Score:", test_f1)
